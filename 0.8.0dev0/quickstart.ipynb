{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37341cca",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "This is a self contained guide on how to write a simple app and start launching\n",
    "distributed jobs on local and remote clusters.\n",
    "\n",
    "## Installation\n",
    "\n",
    "First thing we need to do is to install the TorchX python package which includes\n",
    "the CLI and the library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fa1080",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "```sh\n",
    "# install torchx with all dependencies\n",
    "$ pip install \"torchx[dev]\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d32398",
   "metadata": {},
   "source": [
    "See the [README](https://github.com/meta-pytorch/torchx) for more\n",
    "information on installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af274726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:35:15.814046Z",
     "iopub.status.busy": "2025-10-16T21:35:15.813768Z",
     "iopub.status.idle": "2025-10-16T21:35:16.114510Z",
     "shell.execute_reply": "2025-10-16T21:35:16.113519Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: torchx [-h] [--log_level LOG_LEVEL] [--version]\n",
      "              {builtins,cancel,configure,desc"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ribe,list,log,run,runopts,status,tracker}\n",
      "              ...\n",
      "\n",
      "torchx CLI\n",
      "\n",
      "options:\n",
      "  -h, --help      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      show this help message and exit\n",
      "  --log_level LOG_LEVEL\n",
      "                        Python logging"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " log level\n",
      "  --version             show program's version number and exit\n",
      "\n",
      "sub-commands:\n",
      "  Use the f"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ollowing commands to run operations, e.g.: torchx run ${JOB_NAME}\n",
      "\n",
      "  {builtins,cancel,configure,desc"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ribe,list,log,run,runopts,status,tracker}\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "torchx --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e072be",
   "metadata": {},
   "source": [
    "## Hello World\n",
    "\n",
    "Lets start off with writing a simple \"Hello World\" python app. This is just a\n",
    "normal python program and can contain anything you'd like.\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "<div class=\"admonition-title\">Note</div>\n",
    "This example uses Jupyter Notebook `%%writefile` to create local files for\n",
    "example purposes. Under normal usage you would have these as standalone files.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9ab1c93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:35:16.116388Z",
     "iopub.status.busy": "2025-10-16T21:35:16.116179Z",
     "iopub.status.idle": "2025-10-16T21:35:16.120605Z",
     "shell.execute_reply": "2025-10-16T21:35:16.119703Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting my_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_app.py\n",
    "\n",
    "import sys\n",
    "\n",
    "print(f\"Hello, {sys.argv[1]}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab7a46e",
   "metadata": {},
   "source": [
    "## Launching\n",
    "\n",
    "We can execute our app via `torchx run`. The\n",
    "`local_cwd` scheduler executes the app relative to the current directory.\n",
    "\n",
    "For this we'll use the `utils.python` component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd34f26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:35:16.122272Z",
     "iopub.status.busy": "2025-10-16T21:35:16.122095Z",
     "iopub.status.idle": "2025-10-16T21:35:17.075500Z",
     "shell.execute_reply": "2025-10-16T21:35:17.074444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: torchx run <run args...> python  [--help] [-m str] [-c str]\n",
      "                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       [--script str] [--image str]\n",
      "                                        [--name str] [--cpu int]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [--gpu int]\n",
      "                                        [--memMB int] [-h str]\n",
      "                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                [--num_replicas int]\n",
      "                                        ...\n",
      "\n",
      "Runs ``python`` wi"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th the specified module, command or script on the specified\n",
      "image and host. Use ``--`` to separate c"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omponent args and program args\n",
      "(e.g. ``torchx run utils.python --m foo.main -- --args to --main``)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: (cpu, gpu, memMB) parameters are mutually exclusive with ``h`` (named resource) where\n",
      "      ``"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h`` takes precedence if specified for setting resource requirements.\n",
      "      See `registering named re"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sources <https://meta-pytorch.org/torchx/latest/advanced.html#registering-named-resources>`_.\n",
      "\n",
      "posit"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ional arguments:\n",
      "  str                 arguments passed to the program in sys.argv[1:] (ignored\n",
      "    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  with `--c`) (required)\n",
      "\n",
      "options:\n",
      "  --help              show this help message and "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit\n",
      "  -m str, --m str     run library module as a script (default: None)\n",
      "  -c str, --c str     prog"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ram passed as string (may error if scheduler has a\n",
      "                      length limit on args) (defa"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ult: None)\n",
      "  --script str        .py script to run (default: None)\n",
      "  --image str         image to ru"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n on (default:\n",
      "                      ghcr.io/pytorch/torchx:0.8.0dev0)\n",
      "  --name str          name of"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the job (default: torchx_utils_python)\n",
      "  --cpu int           number of cpus per replica (default: 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n",
      "  --gpu int           number of gpus per replica (default: 0)\n",
      "  --memMB int         cpu memory in "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MB per replica (default: 1024)\n",
      "  -h str, --h str     a registered named resource (if specified takes"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                      precedence over cpu, gpu, memMB) (default: None)\n",
      "  --num_replicas int  number"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of copies to run (each on its own container)\n",
      "                      (default: 1)\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "torchx run --scheduler local_cwd utils.python --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d3687e",
   "metadata": {},
   "source": [
    "The component takes in the script name and any extra arguments will be passed to\n",
    "the script itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe1cb30f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:35:17.077364Z",
     "iopub.status.busy": "2025-10-16T21:35:17.077165Z",
     "iopub.status.idle": "2025-10-16T21:35:19.016121Z",
     "shell.execute_reply": "2025-10-16T21:35:19.015182Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:17 INFO     Tracker configurations: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:17 INFO     Log directory not set in scheduler cfg. Creating a temporary log"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " dir that will be deleted on exit. To preserve log directory set the `log_dir` cfg option\n",
      "torchx 202"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5-10-16 21:35:17 INFO     Log directory is: /tmp/torchx_id2yoth3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:17 INFO     Waiting for the app to finish...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python/0 Hello, your name!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:18 INFO     Job finished: SUCCEEDED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_cwd://torchx/torchx_utils_python-t7nxmggnd79nwc\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "torchx run --scheduler local_cwd utils.python --script my_app.py \"your name\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad38b308",
   "metadata": {},
   "source": [
    "We can run the exact same app via the `local_docker` scheduler. This scheduler\n",
    "will package up the local workspace as a layer on top of the specified image.\n",
    "This provides a very similar environment to the container based remote\n",
    "schedulers.\n",
    "\n",
    "<div class=\"admonition note\">\n",
    "<div class=\"admonition-title\">Note</div>\n",
    "This requires Docker installed and won't work in environments such as Google\n",
    "Colab. See the Docker install instructions:\n",
    "[https://docs.docker.com/get-docker/](https://docs.docker.com/get-docker/)</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "524a526d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:35:19.018327Z",
     "iopub.status.busy": "2025-10-16T21:35:19.018124Z",
     "iopub.status.idle": "2025-10-16T21:35:34.354104Z",
     "shell.execute_reply": "2025-10-16T21:35:34.353242Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:19 INFO     Tracker configurations: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:19 INFO     Checking for changes in workspace `/home/runner/work/torchx/torc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hx/docs/source` for role[0]=python...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:20 INFO     Workspace `/home/runner/work/torchx/torchx/docs/source` resolved"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " to filesystem path `/home/runner/work/torchx/torchx/docs/source`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:20 INFO     Building workspace docker image (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:20 INFO     Step 1/4 : ARG IMAGE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:20 INFO     Step 2/4 : FROM $IMAGE\n",
      "torchx 2025-10-16 21:35:20 INFO      --->"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " da9ca21e8d49\n",
      "torchx 2025-10-16 21:35:20 INFO     Step 3/4 : COPY . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:27 INFO      ---> 1d0a67f665a9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:27 INFO     Step 4/4 : LABEL torchx.pytorch.org/version=0.8.0dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:27 INFO      ---> Running in a682e32d6090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:33 INFO      ---> Removed intermediate container a682e32d6090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:33 INFO      ---> 0a9a7aa570fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:33 INFO     [Warning] One or more build-args [WORKSPACE] were not consumed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:33 INFO     Successfully built 0a9a7aa570fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:33 INFO     Built new image `sha256:0a9a7aa570fa926feabbb9a95dc62985b862d7a3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ec7a634aa96de514f24dec0b` based on original image `ghcr.io/pytorch/torchx:0.8.0dev0` and changes in "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "workspace `/home/runner/work/torchx/torchx/docs/source` for role[0]=python.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:33 INFO     Waiting for the app to finish...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python/0 Hello, your name!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:34 INFO     Job finished: SUCCEEDED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_docker://torchx/torchx_utils_python-m9fm61zhrsj4k\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "torchx run --scheduler local_docker utils.python --script my_app.py \"your name\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d87402",
   "metadata": {},
   "source": [
    "TorchX defaults to using the\n",
    "[ghcr.io/pytorch/torchx](https://ghcr.io/pytorch/torchx) Docker container image\n",
    "which contains the PyTorch libraries, TorchX and related dependencies.\n",
    "\n",
    "## Distributed\n",
    "\n",
    "TorchX's `dist.ddp` component uses\n",
    "[TorchElastic](https://pytorch.org/docs/stable/distributed.elastic.html)\n",
    "to manage the workers. This means you can launch multi-worker and multi-host\n",
    "jobs out of the box on all of the schedulers we support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99ffe3b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:35:34.356518Z",
     "iopub.status.busy": "2025-10-16T21:35:34.356315Z",
     "iopub.status.idle": "2025-10-16T21:35:35.376164Z",
     "shell.execute_reply": "2025-10-16T21:35:35.375196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: torchx run <run args...> ddp  [--help] [--script str] [-m str]\n",
      "                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       [--image str] [--name str] [-h str]\n",
      "                                     [--cpu int] [--gpu i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt] [--memMB int]\n",
      "                                     [-j str] [--env str] [--metadata str]\n",
      "       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              [--max_retries int] [--rdzv_port int]\n",
      "                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     [--rdzv_backend str] [--rdzv_conf str]\n",
      "                                     [--mounts str] [--d"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ebug str] [--tee int]\n",
      "                                     ...\n",
      "\n",
      "Distributed data parallel style appl"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ication (one role, multi-replica).\n",
      "Uses `torch.distributed.run <https://pytorch.org/docs/stable/dist"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ributed.elastic.html>`_\n",
      "to launch and coordinate PyTorch worker processes. Defaults to using ``c10d`"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "` rendezvous backend\n",
      "on rendezvous_endpoint ``$rank_0_host:$rdzv_port``. Note that ``rdzv_port`` par"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ameter is ignored\n",
      "when running on single node, and instead we use port 0 which instructs torchelasti"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c to chose\n",
      "a free random port on the host.\n",
      "\n",
      "Note: (cpu, gpu, memMB) parameters are mutually exclusiv"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e with ``h`` (named resource) where\n",
      "      ``h`` takes precedence if specified for setting resource r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "equirements.\n",
      "      See `registering named resources <https://meta-pytorch.org/torchx/latest/advanced"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".html#registering-named-resources>`_.\n",
      "\n",
      "positional arguments:\n",
      "  str                 arguments to the "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main module (required)\n",
      "\n",
      "options:\n",
      "  --help              show this help message and exit\n",
      "  --script st"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r        script or binary to run within the image (default: None)\n",
      "  -m str, --m str     the python m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odule path to run (default: None)\n",
      "  --image str         image (e.g. docker) (default:\n",
      "              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ghcr.io/pytorch/torchx:0.8.0dev0)\n",
      "  --name str          job name override in the following f"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ormat:\n",
      "                      ``{experimentname}/{runname}`` or ``{experimentname}/``\n",
      "               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       or ``/{runname}`` or ``{runname}``. Uses the script or\n",
      "                      module name if `"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`{runname}`` not specified. (default: /)\n",
      "  -h str, --h str     a registered named resource (if speci"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fied takes\n",
      "                      precedence over cpu, gpu, memMB) (default: None)\n",
      "  --cpu int       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    number of cpus per replica (default: 2)\n",
      "  --gpu int           number of gpus per replica (defaul"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t: 0)\n",
      "  --memMB int         cpu memory in MB per replica (default: 1024)\n",
      "  -j str, --j str     [{min"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_nnodes}:]{nnodes}x{nproc_per_node}, for gpu hosts,\n",
      "                      nproc_per_node must not ex"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ceed num gpus (default: 1x2)\n",
      "  --env str           environment varibles to be passed to the run (e.g"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "                      ENV1=v1,ENV2=v2,ENV3=v3) (default: None)\n",
      "  --metadata str      metadata to b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e passed to the scheduler (e.g.\n",
      "                      KEY1=v1,KEY2=v2,KEY3=v3) (default: None)\n",
      "  --m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ax_retries int   the number of scheduler retries allowed (default: 0)\n",
      "  --rdzv_port int     the port"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " on rank0's host to use for hosting the c10d\n",
      "                      store used for rendezvous. Only t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akes effect when\n",
      "                      running multi-node. When running single node, this\n",
      "          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            parameter is ignored and a random free port is chosen.\n",
      "                      (default: 2"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9500)\n",
      "  --rdzv_backend str  the rendezvous backend to use. Only takes effect when\n",
      "                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    running multi-node. (default: c10d)\n",
      "  --rdzv_conf str     the additional rendezvous configuratio"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n to use (ex.\n",
      "                      join_timeout=600,close_timeout=600,timeout=600).\n",
      "               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       (default: None)\n",
      "  --mounts str        mounts to mount into the worker environment/container\n",
      " "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     (ex. type=<bind/volume>,src=/host,dst=/job[,readonly]).\n",
      "                      S"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ee scheduler documentation for more info. (default:\n",
      "                      None)\n",
      "  --debug str       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  whether to run with preset debug flags enabled (default:\n",
      "                      False)\n",
      "  --tee int "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          tees the specified std stream(s) to console + file. 0:\n",
      "                      none, 1: stdo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ut, 2: stderr, 3: both (default: 3)\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "torchx run --scheduler local_docker dist.ddp --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3c8aaa",
   "metadata": {},
   "source": [
    "Lets create a slightly more interesting app to leverage the TorchX distributed\n",
    "support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5233adbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:35:35.378051Z",
     "iopub.status.busy": "2025-10-16T21:35:35.377856Z",
     "iopub.status.idle": "2025-10-16T21:35:35.382286Z",
     "shell.execute_reply": "2025-10-16T21:35:35.381220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dist_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dist_app.py\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "dist.init_process_group(backend=\"gloo\")\n",
    "print(f\"I am worker {dist.get_rank()} of {dist.get_world_size()}!\")\n",
    "\n",
    "a = torch.tensor([dist.get_rank()])\n",
    "dist.all_reduce(a)\n",
    "print(f\"all_reduce output = {a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66ace59",
   "metadata": {},
   "source": [
    "Let launch a small job with 2 nodes and 2 worker processes per node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd75b297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:35:35.383960Z",
     "iopub.status.busy": "2025-10-16T21:35:35.383780Z",
     "iopub.status.idle": "2025-10-16T21:35:56.278338Z",
     "shell.execute_reply": "2025-10-16T21:35:56.277212Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:36 INFO     Tracker configurations: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:36 INFO     Checking for changes in workspace `/home/runner/work/torchx/torc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hx/docs/source` for role[0]=dist_app...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:36 INFO     Workspace `/home/runner/work/torchx/torchx/docs/source` resolved"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " to filesystem path `/home/runner/work/torchx/torchx/docs/source`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:36 INFO     Building workspace docker image (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:37 INFO     Step 1/4 : ARG IMAGE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:37 INFO     Step 2/4 : FROM $IMAGE\n",
      "torchx 2025-10-16 21:35:37 INFO      --->"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " da9ca21e8d49\n",
      "torchx 2025-10-16 21:35:37 INFO     Step 3/4 : COPY . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:43 INFO      ---> a490b4fd5310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:43 INFO     Step 4/4 : LABEL torchx.pytorch.org/version=0.8.0dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:43 INFO      ---> Running in f32d64de9ff5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:48 INFO      ---> Removed intermediate container f32d64de9ff5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:48 INFO      ---> 6996ba55eef2\n",
      "torchx 2025-10-16 21:35:48 INFO     [Warning]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " One or more build-args [WORKSPACE] were not consumed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:48 INFO     Successfully built 6996ba55eef2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:48 INFO     Built new image `sha256:6996ba55eef2d2e3a13a7bbcc3df811be599e209"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e46cf7cd033ee791e76389cb` based on original image `ghcr.io/pytorch/torchx:0.8.0dev0` and changes in "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "workspace `/home/runner/work/torchx/torchx/docs/source` for role[0]=dist_app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:49 INFO     Waiting for the app to finish...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dist_app/0 W1016 21:35:51.154000 1 site-packages/torch/distributed/run.py:774] \n",
      "dist_app/0 W1016 21:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35:51.154000 1 site-packages/torch/distributed/run.py:774] *****************************************"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dist_app/1 W1016 21:35:51.154000 1 site-packages/torch/distributed/run.py:774] \n",
      "dist_app/1 W1016 21:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35:51.154000 1 site-packages/torch/distributed/run.py:774] *****************************************"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "dist_app/1 W1016 21:35:51.154000 1 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THRE"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ADS environment variable for each process to be 1 in default, to avoid your system being overloaded,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " please further tune the variable for optimal performance in your application as needed. \n",
      "dist_app/1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " W1016 21:35:51.154000 1 site-packages/torch/distributed/run.py:774] *******************************"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "**********\n",
      "dist_app/0 W1016 21:35:51.154000 1 site-packages/torch/distributed/run.py:774] Setting OM"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "P_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being o"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "verloaded, please further tune the variable for optimal performance in your application as needed. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dist_app/0 W1016 21:35:51.154000 1 site-packages/torch/distributed/run.py:774] *********************"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dist_app/1 [0]:[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks i"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s : 3\n",
      "dist_app/1 [0]:I am worker 2 of 4!\n",
      "dist_app/1 [0]:all_reduce output = tensor([6])\n",
      "dist_app/1 ["
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1]:[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "dist_a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pp/1 [1]:I am worker 3 of 4!\n",
      "dist_app/1 [1]:all_reduce output = tensor([6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dist_app/0 [0]:[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks i"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s : 3\n",
      "dist_app/0 [0]:I am worker 0 of 4!\n",
      "dist_app/0 [0]:all_reduce output = tensor([6])\n",
      "dist_app/0 ["
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1]:[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "dist_a"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pp/0 [1]:I am worker 1 of 4!\n",
      "dist_app/0 [1]:all_reduce output = tensor([6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:56 INFO     Job finished: SUCCEEDED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_docker://torchx/dist_app-gtzz6mzl6wxtsc\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "torchx run --scheduler local_docker dist.ddp -j 2x2 --script dist_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df9aa56",
   "metadata": {},
   "source": [
    "## Workspaces / Patching\n",
    "\n",
    "For each scheduler there's a concept of an `image`. For `local_cwd` and `slurm`\n",
    "it uses the current working directory. For container based schedulers such as\n",
    "`local_docker`, `kubernetes` and `aws_batch` it uses a docker container.\n",
    "\n",
    "To provide the same environment between local and remote jobs, TorchX CLI uses\n",
    "workspaces to automatically patch images for remote jobs on a per scheduler\n",
    "basis.\n",
    "\n",
    "When you launch a job via `torchx run` it'll overlay the current directory on\n",
    "top of the provided image so your code is available in the launched job.\n",
    "\n",
    "For `docker` based schedulers you'll need a local docker daemon to build and\n",
    "push the image to your remote docker repository.\n",
    "\n",
    "## `.torchxconfig`\n",
    "\n",
    "Arguments to schedulers can be specified either via a command line flag to\n",
    "`torchx run -s <scheduler> -c <args>` or on a per scheduler basis via a\n",
    "`.torchxconfig` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf0e82f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:35:56.280236Z",
     "iopub.status.busy": "2025-10-16T21:35:56.280044Z",
     "iopub.status.idle": "2025-10-16T21:35:56.284170Z",
     "shell.execute_reply": "2025-10-16T21:35:56.283396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing .torchxconfig\n"
     ]
    }
   ],
   "source": [
    "%%writefile .torchxconfig\n",
    "\n",
    "[kubernetes]\n",
    "queue=torchx\n",
    "image_repo=<your docker image repository>\n",
    "\n",
    "[slurm]\n",
    "partition=torchx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f762615c",
   "metadata": {},
   "source": [
    "## Remote Schedulers\n",
    "\n",
    "TorchX supports a large number of schedulers.\n",
    "Don't see yours?\n",
    "[Request it!](https://github.com/meta-pytorch/torchx/issues/new?assignees=&labels=&template=feature-request.md)\n",
    "\n",
    "Remote schedulers operate the exact same way the local schedulers do. The same\n",
    "run command for local works out of the box on remote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49be1fc",
   "metadata": {
    "region_name": "md"
   },
   "source": [
    "```sh\n",
    "$ torchx run --scheduler slurm dist.ddp -j 2x2 --script dist_app.py\n",
    "$ torchx run --scheduler kubernetes dist.ddp -j 2x2 --script dist_app.py\n",
    "$ torchx run --scheduler aws_batch dist.ddp -j 2x2 --script dist_app.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a326567d",
   "metadata": {},
   "source": [
    "Depending on the scheduler there may be a few extra configuration parameters so\n",
    "TorchX knows where to run the job and upload built images. These can either be\n",
    "set via `-c` or in the `.torchxconfig` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e006b83",
   "metadata": {},
   "source": [
    "All config options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b7c3508",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:35:56.286522Z",
     "iopub.status.busy": "2025-10-16T21:35:56.286112Z",
     "iopub.status.idle": "2025-10-16T21:35:58.221950Z",
     "shell.execute_reply": "2025-10-16T21:35:58.220971Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_docker:\n",
      "    usage:\n",
      "        [copy_env=COPY_ENV],[env=ENV],[privileged=PRIVILEGED],[image_repo=I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAGE_REPO],[quiet=QUIET]\n",
      "\n",
      "    optional arguments:\n",
      "        copy_env=COPY_ENV (typing.List[str], None)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            list of glob patterns of environment variables to copy if not set in AppDef. Ex: FOO_*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        env=ENV (typing.Dict[str, str], None)\n",
      "            environment variables to be passed to the "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run. The separator sign can be eiher comma or semicolon\n",
      "            (e.g. ENV1:v1,ENV2:v2,ENV3:v3 or"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ENV1:V1;ENV2:V2). Environment variables from env will be applied on top\n",
      "            of the ones fro"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m copy_env\n",
      "        privileged=PRIVILEGED (bool, False)\n",
      "            If true runs the container with e"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "levated permissions. Equivalent to running with `docker run --privileged`.\n",
      "        image_repo=IMAGE_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO (str, None)\n",
      "            (remote jobs) the image repository to use when pushing patched images, "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "must have push access. Ex: example.com/your/container\n",
      "        quiet=QUIET (bool, False)\n",
      "            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whether to suppress verbose output for image building. Defaults to ``False``.\n",
      "\n",
      "local_cwd:\n",
      "    usage:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        [log_dir=LOG_DIR],[prepend_cwd=PREPEND_CWD],[auto_set_cuda_visible_devices=AUTO_SET_CUDA_VI"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIBLE_DEVICES]\n",
      "\n",
      "    optional arguments:\n",
      "        log_dir=LOG_DIR (str, None)\n",
      "            dir to write"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " stdout/stderr log files of replicas\n",
      "        prepend_cwd=PREPEND_CWD (bool, False)\n",
      "            if se"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t, prepends CWD to replica's PATH env var making any binaries in CWD take precedence over those in P"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATH\n",
      "        auto_set_cuda_visible_devices=AUTO_SET_CUDA_VISIBLE_DEVICES (bool, False)\n",
      "            se"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts the `CUDA_AVAILABLE_DEVICES` for roles that request GPU resources. Each role replica will be assi"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gned one GPU. Does nothing if the device count is less than replicas.\n",
      "\n",
      "slurm:\n",
      "    usage:\n",
      "        [pa"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rtition=PARTITION],[time=TIME],[comment=COMMENT],[constraint=CONSTRAINT],[mail-user=MAIL-USER],[mail"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-type=MAIL-TYPE],[job_dir=JOB_DIR],[qos=QOS]\n",
      "\n",
      "    optional arguments:\n",
      "        partition=PARTITION (s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr, None)\n",
      "            The partition to run the job in.\n",
      "        time=TIME (str, None)\n",
      "            The"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " maximum time the job is allowed to run for. Formats:             \"minutes\", \"minutes:seconds\", \"hou"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rs:minutes:seconds\", \"days-hours\",             \"days-hours:minutes\" or \"days-hours:minutes:seconds\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        comment=COMMENT (str, None)\n",
      "            Comment to set on the slurm job.\n",
      "        constraint="
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONSTRAINT (str, None)\n",
      "            Constraint to use for the slurm job.\n",
      "        mail-user=MAIL-USER "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(str, None)\n",
      "            User to mail on job end.\n",
      "        mail-type=MAIL-TYPE (str, None)\n",
      "           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " What events to mail users on.\n",
      "        job_dir=JOB_DIR (str, None)\n",
      "            The directory to plac"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e the job code and outputs. The\n",
      "            directory must not exist and will be created. To enable "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log\n",
      "            iteration, jobs will be tracked in ``.torchxslurmjobdirs``.\n",
      "            \n",
      "        qos"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=QOS (str, None)\n",
      "            Quality of Service (QoS) to assign to the job.\n",
      "\n",
      "kubernetes:\n",
      "    usage:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        queue=QUEUE,[namespace=NAMESPACE],[service_account=SERVICE_ACCOUNT],[priority_class=PRIORITY"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CLASS],[image_repo=IMAGE_REPO],[quiet=QUIET]\n",
      "\n",
      "    required arguments:\n",
      "        queue=QUEUE (str)\n",
      "   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Volcano queue to schedule job in\n",
      "\n",
      "    optional arguments:\n",
      "        namespace=NAMESPACE (str,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " default)\n",
      "            Kubernetes namespace to schedule job in\n",
      "        service_account=SERVICE_ACCOUN"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T (str, None)\n",
      "            The service account name to set on the pod specs\n",
      "        priority_class=PR"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IORITY_CLASS (str, None)\n",
      "            The name of the PriorityClass to set on the job specs\n",
      "        i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mage_repo=IMAGE_REPO (str, None)\n",
      "            (remote jobs) the image repository to use when pushing "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patched images, must have push access. Ex: example.com/your/container\n",
      "        quiet=QUIET (bool, Fal"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "se)\n",
      "            whether to suppress verbose output for image building. Defaults to ``False``.\n",
      "\n",
      "kuber"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netes_mcad:\n",
      "    usage:\n",
      "        [namespace=NAMESPACE],[image_repo=IMAGE_REPO],[service_account=SERVIC"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_ACCOUNT],[priority=PRIORITY],[priority_class_name=PRIORITY_CLASS_NAME],[image_secret=IMAGE_SECRET]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",[coscheduler_name=COSCHEDULER_NAME],[network=NETWORK]\n",
      "\n",
      "    optional arguments:\n",
      "        namespace=NA"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MESPACE (str, default)\n",
      "            Kubernetes namespace to schedule job in\n",
      "        image_repo=IMAGE_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO (str, None)\n",
      "            The image repository to use when pushing patched images, must have push"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " access. Ex: example.com/your/container\n",
      "        service_account=SERVICE_ACCOUNT (str, None)\n",
      "        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    The service account name to set on the pod specs\n",
      "        priority=PRIORITY (int, None)\n",
      "         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   The priority level to set on the job specs. Higher integer value means higher priority\n",
      "        pr"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iority_class_name=PRIORITY_CLASS_NAME (str, None)\n",
      "            Pod specific priority level. Check wit"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h your Kubernetes cluster admin if Priority classes are defined on your system\n",
      "        image_secret="
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE_SECRET (str, None)\n",
      "            The name of the Kubernetes/OpenShift secret set up for private "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images\n",
      "        coscheduler_name=COSCHEDULER_NAME (str, None)\n",
      "            Option to run TorchX-MCAD w"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ith a co-scheduler. User must provide the co-scheduler name.\n",
      "        network=NETWORK (str, None)\n",
      "   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Name of additional pod-to-pod network beyond default Kubernetes network\n",
      "\n",
      "aws_batch:\n",
      "    usa"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ge:\n",
      "        queue=QUEUE,[user=USER],[privileged=PRIVILEGED],[share_id=SHARE_ID],[priority=PRIORITY],"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[job_role_arn=JOB_ROLE_ARN],[execution_role_arn=EXECUTION_ROLE_ARN],[ulimits=ULIMITS],[image_repo=IM"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGE_REPO],[quiet=QUIET]\n",
      "\n",
      "    required arguments:\n",
      "        queue=QUEUE (str)\n",
      "            queue to sche"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dule job in\n",
      "\n",
      "    optional arguments:\n",
      "        user=USER (str, runner)\n",
      "            The username to tag"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the job with. `getpass.getuser()` if not specified.\n",
      "        privileged=PRIVILEGED (bool, False)\n",
      "   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         If true runs the container with elevated permissions. Equivalent to running with `docker ru"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n --privileged`.\n",
      "        share_id=SHARE_ID (str, None)\n",
      "            The share identifier for the job."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This must be set if and only if the job queue has a scheduling policy.\n",
      "        priority=PRIORITY (i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nt, 0)\n",
      "            The scheduling priority for the job within the context of share_id. Higher number"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (between 0 and 9999) means higher priority. This will only take effect if the job queue has a sched"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uling policy.\n",
      "        job_role_arn=JOB_ROLE_ARN (str, None)\n",
      "            The Amazon Resource Name (AR"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N) of the IAM role that the container can assume for AWS permissions.\n",
      "        execution_role_arn=EXE"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUTION_ROLE_ARN (str, None)\n",
      "            The Amazon Resource Name (ARN) of the IAM role that the ECS "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent can assume for AWS permissions.\n",
      "        ulimits=ULIMITS (typing.List[str], None)\n",
      "            U"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "limit settings in format: name:softLimit:hardLimit (multiple separated by commas)\n",
      "        image_repo"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=IMAGE_REPO (str, None)\n",
      "            (remote jobs) the image repository to use when pushing patched i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mages, must have push access. Ex: example.com/your/container\n",
      "        quiet=QUIET (bool, False)\n",
      "     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       whether to suppress verbose output for image building. Defaults to ``False``.\n",
      "\n",
      "sagemaker.conf"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ig INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config I"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NFO - Not applying SDK defaults from location: /home/runner/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aws_sagemaker:\n",
      "    usage:\n",
      "        role=ROLE,instance_type=INSTANCE_TYPE,[instance_count=INSTANCE_COU"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NT],[user=USER],[keep_alive_period_in_seconds=KEEP_ALIVE_PERIOD_IN_SECONDS],[volume_size=VOLUME_SIZE"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "],[volume_kms_key=VOLUME_KMS_KEY],[max_run=MAX_RUN],[input_mode=INPUT_MODE],[output_path=OUTPUT_PATH"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "],[output_kms_key=OUTPUT_KMS_KEY],[base_job_name=BASE_JOB_NAME],[tags=TAGS],[subnets=SUBNETS],[secur"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ity_group_ids=SECURITY_GROUP_IDS],[model_uri=MODEL_URI],[model_channel_name=MODEL_CHANNEL_NAME],[met"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ric_definitions=METRIC_DEFINITIONS],[encrypt_inter_container_traffic=ENCRYPT_INTER_CONTAINER_TRAFFIC"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "],[use_spot_instances=USE_SPOT_INSTANCES],[max_wait=MAX_WAIT],[checkpoint_s3_uri=CHECKPOINT_S3_URI],"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[checkpoint_local_path=CHECKPOINT_LOCAL_PATH],[debugger_hook_config=DEBUGGER_HOOK_CONFIG],[enable_sa"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemaker_metrics=ENABLE_SAGEMAKER_METRICS],[enable_network_isolation=ENABLE_NETWORK_ISOLATION],[disab"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le_profiler=DISABLE_PROFILER],[environment=ENVIRONMENT],[max_retry_attempts=MAX_RETRY_ATTEMPTS],[sou"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rce_dir=SOURCE_DIR],[git_config=GIT_CONFIG],[hyperparameters=HYPERPARAMETERS],[container_log_level=C"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONTAINER_LOG_LEVEL],[code_location=CODE_LOCATION],[dependencies=DEPENDENCIES],[training_repository_a"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ccess_mode=TRAINING_REPOSITORY_ACCESS_MODE],[training_repository_credentials_provider_arn=TRAINING_R"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOSITORY_CREDENTIALS_PROVIDER_ARN],[disable_output_compression=DISABLE_OUTPUT_COMPRESSION],[enable_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infra_check=ENABLE_INFRA_CHECK],[image_repo=IMAGE_REPO],[quiet=QUIET]\n",
      "\n",
      "    required arguments:\n",
      "     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   role=ROLE (str)\n",
      "            an AWS IAM role (either name or full ARN). The Amazon SageMaker train"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ing jobs and APIs that create Amazon SageMaker endpoints use this role to access training data and m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odel artifacts. After the endpoint is created, the inference code might use the IAM role, if it need"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s to access an AWS resource.\n",
      "        instance_type=INSTANCE_TYPE (str)\n",
      "            type of EC2 insta"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nce to use for training, for example, 'ml.c4.xlarge'\n",
      "\n",
      "    optional arguments:\n",
      "        instance_count"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=INSTANCE_COUNT (int, 1)\n",
      "            number of Amazon EC2 instances to use for training. Required if"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " instance_groups is not set.\n",
      "        user=USER (str, runner)\n",
      "            the username to tag the job"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " with. `getpass.getuser()` if not specified.\n",
      "        keep_alive_period_in_seconds=KEEP_ALIVE_PERIOD_"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_SECONDS (int, None)\n",
      "            the duration of time in seconds to retain configured resources in"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " a warm pool for subsequent training jobs.\n",
      "        volume_size=VOLUME_SIZE (int, None)\n",
      "            s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ize in GB of the storage volume to use for storing input and output data during training (default: 3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0).\n",
      "        volume_kms_key=VOLUME_KMS_KEY (str, None)\n",
      "            KMS key ID for encrypting EBS volu"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "me attached to the training instance.\n",
      "        max_run=MAX_RUN (int, None)\n",
      "            timeout in sec"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onds for training (default: 24 * 60 * 60).\n",
      "        input_mode=INPUT_MODE (str, None)\n",
      "            the"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input mode that the algorithm supports (default: ‘File’).\n",
      "        output_path=OUTPUT_PATH (str,"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " None)\n",
      "            S3 location for saving the training result (model artifacts and output files). If"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " not specified, results are stored to a default bucket. If the bucket with the specific name does no"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t exist, the estimator creates the bucket during the fit() method execution.\n",
      "        output_kms_key="
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT_KMS_KEY (str, None)\n",
      "            KMS key ID for encrypting the training output (default: Your "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAM role’s KMS key for Amazon S3).\n",
      "        base_job_name=BASE_JOB_NAME (str, None)\n",
      "            pre"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix for training job name when the fit() method launches. If not specified, the estimator generates "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a default job name based on the training image name and current timestamp.\n",
      "        tags=TAGS (typing"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".List[typing.Dict[str, str]], None)\n",
      "            list of tags for labeling a training job.\n",
      "        su"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnets=SUBNETS (typing.List[str], None)\n",
      "            list of subnet ids. If not specified training job"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " will be created without VPC config.\n",
      "        security_group_ids=SECURITY_GROUP_IDS (typing.List[str]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", None)\n",
      "            list of security group ids. If not specified training job will be created withou"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t VPC config.\n",
      "        model_uri=MODEL_URI (str, None)\n",
      "            URI where a pre-trained model is s"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tored, either locally or in S3.\n",
      "        model_channel_name=MODEL_CHANNEL_NAME (str, None)\n",
      "          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name of the channel where ‘model_uri’ will be downloaded (default: ‘model’).\n",
      "        metri"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_definitions=METRIC_DEFINITIONS (typing.List[typing.Dict[str, str]], None)\n",
      "            list of dict"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ionaries that defines the metric(s) used to evaluate the training jobs. Each dictionary contains two"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " keys: ‘Name’ for the name of the metric, and ‘Regex’ for the regular expression used to ext"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ract the metric from the logs.\n",
      "        encrypt_inter_container_traffic=ENCRYPT_INTER_CONTAINER_TRAFF"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC (bool, None)\n",
      "            specifies whether traffic between training containers is encrypted for t"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he training job (default: False).\n",
      "        use_spot_instances=USE_SPOT_INSTANCES (bool, None)\n",
      "       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     specifies whether to use SageMaker Managed Spot instances for training. If enabled then the max"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_wait arg should also be set.\n",
      "        max_wait=MAX_WAIT (int, None)\n",
      "            timeout in seconds w"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aiting for spot training job.\n",
      "        checkpoint_s3_uri=CHECKPOINT_S3_URI (str, None)\n",
      "            S3"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " URI in which to persist checkpoints that the algorithm persists (if any) during training.\n",
      "        c"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heckpoint_local_path=CHECKPOINT_LOCAL_PATH (str, None)\n",
      "            local path that the algorithm wri"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tes its checkpoints to.\n",
      "        debugger_hook_config=DEBUGGER_HOOK_CONFIG (bool, None)\n",
      "            c"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onfiguration for how debugging information is emitted with SageMaker Debugger. If not specified, a d"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efault one is created using the estimator’s output_path, unless the region does not support SageMa"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ker Debugger. To disable SageMaker Debugger, set this parameter to False.\n",
      "        enable_sagemaker_m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etrics=ENABLE_SAGEMAKER_METRICS (bool, None)\n",
      "            enable SageMaker Metrics Time Series.\n",
      "     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   enable_network_isolation=ENABLE_NETWORK_ISOLATION (bool, None)\n",
      "            specifies whether cont"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ainer will run in network isolation mode (default: False).\n",
      "        disable_profiler=DISABLE_PROFILER"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (bool, None)\n",
      "            specifies whether Debugger monitoring and profiling will be disabled (defa"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ult: False).\n",
      "        environment=ENVIRONMENT (typing.Dict[str, str], None)\n",
      "            environment v"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ariables to be set for use during training job\n",
      "        max_retry_attempts=MAX_RETRY_ATTEMPTS (int, N"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one)\n",
      "            number of times to move a job to the STARTING status. You can specify between 1 and"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 30 attempts.\n",
      "        source_dir=SOURCE_DIR (str, None)\n",
      "            absolute, relative, or S3 URI Pa"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th to a directory with any other training source code dependencies aside from the entry point file ("
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default: current working directory)\n",
      "        git_config=GIT_CONFIG (typing.Dict[str, str], None)\n",
      "    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        git configurations used for cloning files, including repo, branch, commit, 2FA_enabled, user"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name, password, and token.\n",
      "        hyperparameters=HYPERPARAMETERS (typing.Dict[str, str], None)\n",
      "   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         dictionary containing the hyperparameters to initialize this estimator with.\n",
      "        contai"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_log_level=CONTAINER_LOG_LEVEL (int, None)\n",
      "            log level to use within the container (def"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ault: logging.INFO).\n",
      "        code_location=CODE_LOCATION (str, None)\n",
      "            S3 prefix URI where"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " custom code is uploaded.\n",
      "        dependencies=DEPENDENCIES (typing.List[str], None)\n",
      "            lis"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t of absolute or relative paths to directories with any additional libraries that should be exported"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " to the container.\n",
      "        training_repository_access_mode=TRAINING_REPOSITORY_ACCESS_MODE (str, Non"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e)\n",
      "            specifies how SageMaker accesses the Docker image that contains the training algorith"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m.\n",
      "        training_repository_credentials_provider_arn=TRAINING_REPOSITORY_CREDENTIALS_PROVIDER_ARN"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (str, None)\n",
      "            Amazon Resource Name (ARN) of an AWS Lambda function that provides credenti"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "als to authenticate to the private Docker registry where your training image is hosted.\n",
      "        disa"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ble_output_compression=DISABLE_OUTPUT_COMPRESSION (bool, None)\n",
      "            when set to true, Model i"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s uploaded to Amazon S3 without compression after training finishes.\n",
      "        enable_infra_check=ENAB"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LE_INFRA_CHECK (bool, None)\n",
      "            specifies whether it is running Sagemaker built-in infra che"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ck jobs.\n",
      "        image_repo=IMAGE_REPO (str, None)\n",
      "            (remote jobs) the image repository to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " use when pushing patched images, must have push access. Ex: example.com/your/container\n",
      "        quie"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=QUIET (bool, False)\n",
      "            whether to suppress verbose output for image building. Defaults to"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ``False``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "lsf:\n",
      "    usage:\n",
      "        [lsf_queue=LSF_QUEUE],[jobdir=JOBDIR],[container_workdir=CONTAINER_WORKDIR]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",[host_network=HOST_NETWORK],[shm_size=SHM_SIZE]\n",
      "\n",
      "    optional arguments:\n",
      "        lsf_queue=LSF_QUEU"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E (str, None)\n",
      "            queue name to submit jobs\n",
      "        jobdir=JOBDIR (str, None)\n",
      "            Th"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e directory to place the job code and outputs. The directory must not exist and will be created.\n",
      "   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     container_workdir=CONTAINER_WORKDIR (str, None)\n",
      "            working directory in container jobs"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        host_network=HOST_NETWORK (bool, False)\n",
      "            True if using the host network for jobs"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        shm_size=SHM_SIZE (str, 64m)\n",
      "            size of shared memory (/dev/shm) for jobs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "torchx runopts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad85a7a2",
   "metadata": {},
   "source": [
    "## Custom Images\n",
    "\n",
    "### Docker-based Schedulers\n",
    "\n",
    "If you want more than the standard PyTorch libraries you can add custom\n",
    "Dockerfile or build your own docker container and use it as the base image for\n",
    "your TorchX jobs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01b34735",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:35:58.224224Z",
     "iopub.status.busy": "2025-10-16T21:35:58.224010Z",
     "iopub.status.idle": "2025-10-16T21:35:58.228220Z",
     "shell.execute_reply": "2025-10-16T21:35:58.227398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing timm_app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile timm_app.py\n",
    "\n",
    "import timm\n",
    "\n",
    "print(timm.models.resnet18())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb4bd19",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:35:58.229981Z",
     "iopub.status.busy": "2025-10-16T21:35:58.229761Z",
     "iopub.status.idle": "2025-10-16T21:35:58.233344Z",
     "shell.execute_reply": "2025-10-16T21:35:58.232587Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Dockerfile.torchx\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile.torchx\n",
    "\n",
    "FROM pytorch/pytorch:1.10.0-cuda11.3-cudnn8-runtime\n",
    "\n",
    "RUN pip install timm\n",
    "\n",
    "COPY . ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e683aad",
   "metadata": {},
   "source": [
    "Once we have the Dockerfile created we can launch as normal and TorchX will\n",
    "automatically build the image with the newly provided Dockerfile instead of the\n",
    "default one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e200677",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T21:35:58.235288Z",
     "iopub.status.busy": "2025-10-16T21:35:58.235109Z",
     "iopub.status.idle": "2025-10-16T21:37:54.701955Z",
     "shell.execute_reply": "2025-10-16T21:37:54.701089Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:58 INFO     loaded configs from /home/runner/work/torchx/torchx/docs/source/"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".torchxconfig\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:59 INFO     Tracker configurations: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:59 INFO     Checking for changes in workspace `/home/runner/work/torchx/torc"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hx/docs/source` for role[0]=python...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:59 INFO     Workspace `/home/runner/work/torchx/torchx/docs/source` resolved"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " to filesystem path `/home/runner/work/torchx/torchx/docs/source`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:59 INFO     Building workspace docker image (this may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:35:59 INFO     Step 1/4 : FROM pytorch/pytorch:1.10.0-cuda11.3-cudnn8-runtime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:33 INFO      ---> c3f17e5ac010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:33 INFO     Step 2/4 : RUN pip install timm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:33 INFO      ---> Running in 6782faf1bc25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:34 INFO     Collecting timm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:34 INFO       Downloading timm-0.9.12-py3-none-any.whl (2.2 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:34 INFO     Requirement already satisfied: torchvision in /opt/conda/lib/pyt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "hon3.7/site-packages (from timm) (0.11.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:34 INFO     Requirement already satisfied: pyyaml in /opt/conda/lib/python3."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7/site-packages (from timm) (5.4.1)\n",
      "torchx 2025-10-16 21:37:34 INFO     Requirement already satisfie"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d: torch>=1.7 in /opt/conda/lib/python3.7/site-packages (from timm) (1.10.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:34 INFO     Collecting huggingface-hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:34 INFO       Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:35 INFO     Collecting safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:35 INFO       Downloading safetensors-0.5.3.tar.gz (67 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:35 INFO       Installing build dependencies: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:37 INFO       Installing build dependencies: finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:37 INFO       Getting requirements to build wheel: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:37 INFO       Getting requirements to build wheel: finished with status 'don"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:37 INFO       Installing backend dependencies: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:37 INFO       Installing backend dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:37 INFO     \u001b[91m  ERROR: Command errored out with exit status 1:\n",
      "   command"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": /opt/conda/bin/python /opt/conda/lib/python3.7/site-packages/pip install --ignore-installed --no-u"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ser --prefix /tmp/pip-build-env-5rcc4f1m/normal --no-warn-script-location --no-binary :none: --only-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "binary :none: -i https://pypi.org/simple -- puccinialin\n",
      "       cwd: None\n",
      "  Complete output (2 lines)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":\n",
      "  ERROR: Could not find a version that satisfies the requirement puccinialin\n",
      "  ERROR: No matching "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distribution found for puccinialin\n",
      "  ----------------------------------------\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:37 INFO     \u001b[91mWARNING: Discarding https://files.pythonhosted.org/packages"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/71/7e/2d5d6ee7b40c0682315367ec7475693d110f512922d582fef1bd4a63adc3/safetensors-0.5.3.tar.gz#sha256="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b6b0d6ecacec39a4fdd99cc19f4576f5219ce858e6fd8dbe7609df0b8dc56965 (from https://pypi.org/simple/safet"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ensors/) (requires-python:>=3.7). Command errored out with exit status 1: /opt/conda/bin/python /opt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/lib/python3.7/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "env-5rcc4f1m/normal --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pyp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i.org/simple -- puccinialin Check the logs for full command output.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:37 INFO       Downloading safetensors-0.5.2.tar.gz (66 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:37 INFO       Installing build dependencies: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:39 INFO       Installing build dependencies: finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:39 INFO       Getting requirements to build wheel: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:39 INFO       Getting requirements to build wheel: finished with status 'don"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:39 INFO       Installing backend dependencies: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:39 INFO       Installing backend dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:39 INFO     \u001b[91m  ERROR: Command errored out with exit status 1:\n",
      "   command"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": /opt/conda/bin/python /opt/conda/lib/python3.7/site-packages/pip install --ignore-installed --no-u"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ser --prefix /tmp/pip-build-env-_l9x1nbc/normal --no-warn-script-location --no-binary :none: --only-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "binary :none: -i https://pypi.org/simple -- puccinialin\n",
      "       cwd: None\n",
      "  Complete output (2 lines)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":\n",
      "  ERROR: Could not find a version that satisfies the requirement puccinialin\n",
      "  ERROR: No matching "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distribution found for puccinialin\n",
      "  ----------------------------------------\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:39 INFO     \u001b[91mWARNING: Discarding https://files.pythonhosted.org/packages"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/f4/4f/2ef9ef1766f8c194b01b67a63a444d2e557c8fe1d82faf3ebd85f370a917/safetensors-0.5.2.tar.gz#sha256="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cb4a8d98ba12fa016f4241932b1fc5e702e5143f5374bba0bbcf7ddc1c4cf2b8 (from https://pypi.org/simple/safet"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ensors/) (requires-python:>=3.7). Command errored out with exit status 1: /opt/conda/bin/python /opt"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/conda/lib/python3.7/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "env-_l9x1nbc/normal --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pyp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "i.org/simple -- puccinialin Check the logs for full command output.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:39 INFO       Downloading safetensors-0.5.1.tar.gz (66 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:39 INFO       Installing build dependencies: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:41 INFO       Installing build dependencies: finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:41 INFO       Getting requirements to build wheel: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:41 INFO       Getting requirements to build wheel: finished with status 'don"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:41 INFO       Installing backend dependencies: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:41 INFO       Installing backend dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:41 INFO     \u001b[91m  ERROR: Command errored out with exit status 1:\n",
      "   command"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": /opt/conda/bin/python /opt/conda/lib/python3.7/site-packages/pip install --ignore-installed --no-u"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ser --prefix /tmp/pip-build-env-6as40akx/normal --no-warn-script-location --no-binary :none: --only-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "binary :none: -i https://pypi.org/simple -- puccinialin\n",
      "       cwd: None\n",
      "  Complete output (2 lines)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":\n",
      "  ERROR: Could not find a version that satisfies the requirement puccinialin\n",
      "  ERROR: No matching "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distribution found for puccinialin\n",
      "  ----------------------------------------\n",
      "\u001b[0m\n",
      "torchx 2025-10-16"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21:37:41 INFO     \u001b[91mWARNING: Discarding https://files.pythonhosted.org/packages/cd/4b/a45aedf237"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5cd86314749ad4760545b6e4ec7b306cfa142776daaca6c980/safetensors-0.5.1.tar.gz#sha256=75927919a73b0f34d"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6943b531d757f724e65797a900d88d8081fe8b4448eadc3 (from https://pypi.org/simple/safetensors/) (require"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s-python:>=3.7). Command errored out with exit status 1: /opt/conda/bin/python /opt/conda/lib/python"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.7/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-6as40akx/norm"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "al --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "uccinialin Check the logs for full command output.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:41 INFO       Downloading safetensors-0.5.0.tar.gz (65 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:41 INFO       Installing build dependencies: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO       Installing build dependencies: finished with status 'done'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO       Getting requirements to build wheel: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO       Getting requirements to build wheel: finished with status 'don"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO       Installing backend dependencies: started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO       Installing backend dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO     \u001b[91m  ERROR: Command errored out with exit status 1:\n",
      "   command"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": /opt/conda/bin/python /opt/conda/lib/python3.7/site-packages/pip install --ignore-installed --no-u"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ser --prefix /tmp/pip-build-env-bnsoisoo/normal --no-warn-script-location --no-binary :none: --only-"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "binary :none: -i https://pypi.org/simple -- puccinialin\n",
      "       cwd: None\n",
      "  Complete output (2 lines)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":\n",
      "  ERROR: Could not find a version that satisfies the requirement puccinialin\n",
      "  ERROR: No matching "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distribution found for puccinialin\n",
      "  ----------------------------------------\n",
      "\u001b[0m\n",
      "torchx 2025-10-16"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21:37:43 INFO     \u001b[91mWARNING: Discarding https://files.pythonhosted.org/packages/5d/b3/1d9000e9d0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "470499d124ca63c6908f8092b528b48bd95ba11507e14d9dba/safetensors-0.5.0.tar.gz#sha256=c47b34c549fa1e0c6"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55c4644da31332c61332c732c47c8dd9399347e9aac69d1 (from https://pypi.org/simple/safetensors/) (require"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s-python:>=3.7). Command errored out with exit status 1: /opt/conda/bin/python /opt/conda/lib/python"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.7/site-packages/pip install --ignore-installed --no-user --prefix /tmp/pip-build-env-bnsoisoo/norm"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "al --no-warn-script-location --no-binary :none: --only-binary :none: -i https://pypi.org/simple -- p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "uccinialin Check the logs for full command output.\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO       Downloading safetensors-0.4.5-cp37-cp37m-manylinux_2_17_x86_64"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".manylinux2014_x86_64.whl (436 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO     Requirement already satisfied: typing_extensions in /opt/conda/l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ib/python3.7/site-packages (from torch>=1.7->timm) (3.10.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO     Requirement already satisfied: requests in /opt/conda/lib/python"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.7/site-packages (from huggingface-hub->timm) (2.25.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO     Requirement already satisfied: filelock in /opt/conda/lib/python"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3.7/site-packages (from huggingface-hub->timm) (3.0.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO     Collecting packaging>=20.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO       Downloading packaging-24.0-py3-none-any.whl (53 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO     Collecting importlib-metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO       Downloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO     Collecting fsspec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO       Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:43 INFO     Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/py"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thon3.7/site-packages (from huggingface-hub->timm) (4.61.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:44 INFO     Collecting zipp>=0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:44 INFO       Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:44 INFO     Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/l"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ib/python3.7/site-packages (from requests->huggingface-hub->timm) (4.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:44 INFO     Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2021.10.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:44 INFO     Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/con"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "da/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (1.26.6)\n",
      "torchx 2025-10-16 21:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37:44 INFO     Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " (from requests->huggingface-hub->timm) (2.10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:44 INFO     Requirement already satisfied: numpy in /opt/conda/lib/python3.7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/site-packages (from torchvision->timm) (1.21.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:44 INFO     Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /opt/con"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "da/lib/python3.7/site-packages (from torchvision->timm) (8.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:44 INFO     Installing collected packages: zipp, packaging, importlib-metada"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ta, fsspec, safetensors, huggingface-hub, timm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:45 INFO     Successfully installed fsspec-2023.1.0 huggingface-hub-0.16.4 im"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "portlib-metadata-6.7.0 packaging-24.0 safetensors-0.4.5 timm-0.9.12 zipp-3.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:47 INFO      ---> Removed intermediate container 6782faf1bc25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:47 INFO      ---> c60df7280e18\n",
      "torchx 2025-10-16 21:37:47 INFO     Step 3/4 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": COPY . .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:50 INFO      ---> 2d92360f9aa5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:50 INFO     Step 4/4 : LABEL torchx.pytorch.org/version=0.8.0dev0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:50 INFO      ---> Running in ec8d406190e6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:52 INFO      ---> Removed intermediate container ec8d406190e6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:52 INFO      ---> adf0aff662ba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:52 INFO     [Warning] One or more build-args [IMAGE WORKSPACE] were not cons"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "umed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:52 INFO     Successfully built adf0aff662ba\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:52 INFO     Built new image `sha256:adf0aff662ba1d39c9c8218b245ad1e96d08b9f8"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9e1c9b2461b67348732fc949` based on original image `ghcr.io/pytorch/torchx:0.8.0dev0` and changes in "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "workspace `/home/runner/work/torchx/torchx/docs/source` for role[0]=python.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:52 INFO     Waiting for the app to finish...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python/0 ResNet(\n",
      "python/0   (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python/0   (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python/0   (act1): ReLU(inplace=True)\n",
      "python/0   (maxpool): MaxPool2d(kernel_size=3, stride=2, paddi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ng=1, dilation=1, ceil_mode=False)\n",
      "python/0   (layer1): Sequential(\n",
      "python/0     (0): BasicBlock(\n",
      "py"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thon/0       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python/0       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ")\n",
      "python/0       (drop_block): Identity()\n",
      "python/0       (act1): ReLU(inplace=True)\n",
      "python/0       ("
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "aa): Identity()\n",
      "python/0       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", 1), bias=False)\n",
      "python/0       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "running_stats=True)\n",
      "python/0       (act2): ReLU(inplace=True)\n",
      "python/0     )\n",
      "python/0     (1): Basic"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Block(\n",
      "python/0       (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bia"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s=False)\n",
      "python/0       (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tats=True)\n",
      "python/0       (drop_block): Identity()\n",
      "python/0       (act1): ReLU(inplace=True)\n",
      "python/"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0       (aa): Identity()\n",
      "python/0       (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "adding=(1, 1), bias=False)\n",
      "python/0       (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=Tru"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, track_running_stats=True)\n",
      "python/0       (act2): ReLU(inplace=True)\n",
      "python/0     )\n",
      "python/0   )\n",
      "p"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ython/0   (layer2): Sequential(\n",
      "python/0     (0): BasicBlock(\n",
      "python/0       (conv1): Conv2d(64, 128"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ", kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "python/0       (bn1): BatchNorm2d(1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "python/0       (drop_block): Ide"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntity()\n",
      "python/0       (act1): ReLU(inplace=True)\n",
      "python/0       (aa): Identity()\n",
      "python/0       (co"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "python/0      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "python/0   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    (act2): ReLU(inplace=True)\n",
      "python/0       (downsample): Sequential(\n",
      "python/0         (0): Conv2d"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "python/0         (1): BatchNorm2d(128, eps="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "python/0       )\n",
      "python/0     )\n",
      "python/0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     (1): BasicBlock(\n",
      "python/0       (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), pa"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dding=(1, 1), bias=False)\n",
      "python/0       (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=Tru"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e, track_running_stats=True)\n",
      "python/0       (drop_block): Identity()\n",
      "python/0       (act1): ReLU(inp"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lace=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python/0       (aa): Identity()\n",
      "python/0       (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(1, 1), padding=(1, 1), bias=False)\n",
      "python/0       (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "affine=True, track_running_stats=True)\n",
      "python/0       (act2): ReLU(inplace=True)\n",
      "python/0     )\n",
      "pyth"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "on/0   )\n",
      "python/0   (layer3): Sequential(\n",
      "python/0     (0): BasicBlock(\n",
      "python/0       (conv1): Conv"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "python/0       (bn1): Ba"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "python/0       (drop_"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "block): Identity()\n",
      "python/0       (act1): ReLU(inplace=True)\n",
      "python/0       (aa): Identity()\n",
      "python/"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0       (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python/0       (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=Tru"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e)\n",
      "python/0       (act2): ReLU(inplace=True)\n",
      "python/0       (downsample): Sequential(\n",
      "python/0      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "python/0         (1): BatchN"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "orm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "python/0       )\n",
      "python/0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     )\n",
      "python/0     (1): BasicBlock(\n",
      "python/0       (conv1): Conv2d(256, 256, kernel_size=(3, 3), st"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ride=(1, 1), padding=(1, 1), bias=False)\n",
      "python/0       (bn1): BatchNorm2d(256, eps=1e-05, momentum="
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.1, affine=True, track_running_stats=True)\n",
      "python/0       (drop_block): Identity()\n",
      "python/0       ("
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "act1): ReLU(inplace=True)\n",
      "python/0       (aa): Identity()\n",
      "python/0       (conv2): Conv2d(256, 256, k"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "python/0       (bn2): BatchNorm2d(256,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "python/0       (act2): ReLU(inplace"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=True)\n",
      "python/0     )\n",
      "python/0   )\n",
      "python/0   (layer4): Sequential(\n",
      "python/0     (0): BasicBlock(\n",
      "py"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thon/0       (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ")\n",
      "python/0       (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=T"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rue)\n",
      "python/0       (drop_block): Identity()\n",
      "python/0       (act1): ReLU(inplace=True)\n",
      "python/0     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  (aa): Identity()\n",
      "python/0       (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), paddi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ng=(1, 1), bias=False)\n",
      "python/0       (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "track_running_stats=True)\n",
      "python/0       (act2): ReLU(inplace=True)\n",
      "python/0       (downsample): Seq"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "uential(\n",
      "python/0         (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "pytho"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "n/0         (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "py"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "thon/0       )\n",
      "python/0     )\n",
      "python/0     (1): BasicBlock(\n",
      "python/0       (conv1): Conv2d(512, 512,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "python/0       (bn1): BatchNorm2d(51"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "python/0       (drop_block): Iden"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tity()\n",
      "python/0       (act1): ReLU(inplace=True)\n",
      "python/0       (aa): Identity()\n",
      "python/0       (con"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "v2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "python/0       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "python/0    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   (act2): ReLU(inplace=True)\n",
      "python/0     )\n",
      "python/0   )\n",
      "python/0   (global_pool): SelectAdaptivePo"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ol2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
      "python/0   (fc): Linear(in_features=51"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2, out_features=1000, bias=True)\n",
      "python/0 )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchx 2025-10-16 21:37:54 INFO     Job finished: SUCCEEDED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_docker://torchx/torchx_utils_python-g2f5x4mkfcsp6\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "torchx run --scheduler local_docker utils.python --script timm_app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514cc39f",
   "metadata": {},
   "source": [
    "### Slurm\n",
    "\n",
    "The `slurm` and `local_cwd` use the current environment so you can use `pip` and\n",
    "`conda` as normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298b7b98",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Checkout other features of the [torchx CLI](cli.rst)\n",
    "2. Take a look at the [list of schedulers](schedulers.rst) supported by the runner\n",
    "3. Browse through the collection of [builtin components](components/overview.rst)\n",
    "5. See a [training app example](examples_apps/index.rst)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown",
    "format_version": "1.1",
    "jupytext_version": "1.1.0"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
